# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y97iTzowXFxJYP62fxd_JvfjjLFyaLY6
"""

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Sample text data
texts = ["I love deep learning", "Deep learning is fascinating", "I enjoy learning new things"]

# Tokenize the text
tokenizer = Tokenizer(char_level=False)  # Use char_level=True for character-based tokenization
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# Create sequences for LSTM
look_back = 3
X, y = [], []
for seq in sequences:
    for i in range(len(seq) - look_back):
        X.append(seq[i:i + look_back])
        y.append(seq[i + look_back])

X = np.array(X)
y = np.array(y)

sequences, X, y
# Pad sequences
X = pad_sequences(X, maxlen=look_back)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding

# Define model
vocab_size = len(tokenizer.word_index) + 1  # Include the padding token
embedding_dim = 50

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=look_back))
model.add(LSTM(100, return_sequences=True))
model.add(LSTM(100))
model.add(Dense(vocab_size, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
model.summary()

# Train the model
model.fit(X, y, epochs=20, verbose=2)

def generate_text(seed_text, next_words, model, tokenizer, look_back):
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=look_back)
        predicted_probs = model.predict(token_list, verbose=0)
        predicted = np.argmax(predicted_probs, axis=-1)
        output_word = tokenizer.index_word.get(predicted[0])
        seed_text += " " + output_word
    return seed_text

seed_text = "I love"
generated_text = generate_text(seed_text, 5, model, tokenizer, look_back)
print(generated_text)
